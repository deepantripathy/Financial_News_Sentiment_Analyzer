{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from collections import deque\n",
    "import scrapy\n",
    "from scrapy.http import HtmlResponse\n",
    "from scrapy.selector import Selector\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "import sqlite3\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_seed_urls_from_csv(csv_file):\n",
    "    seed_urls = []\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Assuming the URLs are in the first column of the CSV file\n",
    "            if row:\n",
    "                seed_urls.append(row[0].strip())\n",
    "    return seed_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information(web_page_content):\n",
    "    # Initialize Scrapy selector with the web page content\n",
    "    selector = Selector(text=web_page_content)\n",
    "\n",
    "    # Extract relevant information using Scrapy selectors and XPath/CSS\n",
    "    title = selector.xpath(\"//title/text()\").get()\n",
    "    publication_date = selector.xpath(\"//meta[@name='publication_date']/@content\").get()\n",
    "    author = selector.xpath(\"//meta[@name='author']/@content\").get()\n",
    "    content = selector.xpath(\"//div[@class='article-content']//p//text()\").getall()\n",
    "\n",
    "    # Extract text from figures using OCR\n",
    "    figure_texts = []\n",
    "    figures = selector.xpath(\"//div[@class='figure']\")\n",
    "    for figure in figures:\n",
    "        # Assuming the figure is an image\n",
    "        image_url = figure.xpath(\".//@src\").get()\n",
    "        if image_url:\n",
    "            try:\n",
    "                # Send a request to the image URL and retrieve the image content\n",
    "                image_response = requests.get(image_url)\n",
    "                if image_response.status_code == 200:\n",
    "                    # Convert image content to PIL Image\n",
    "                    image = Image.open(BytesIO(image_response.content))\n",
    "\n",
    "                    # Perform OCR on the image to extract text\n",
    "                    figure_text = pytesseract.image_to_string(image)\n",
    "                    figure_texts.append(figure_text)\n",
    "            except Exception as e:\n",
    "                # Handle any exceptions that might occur during image processing or OCR\n",
    "                print(f\"Error processing image: {image_url}\")\n",
    "                print(e)\n",
    "\n",
    "    table_data = []\n",
    "    table_rows = selector.xpath(\"//table[@class='data-table']//tr\")\n",
    "    for row in table_rows:\n",
    "        row_data = row.xpath(\".//td//text()\").getall()\n",
    "        table_data.append(row_data)\n",
    "\n",
    "    # Return the extracted data as a dictionary or relevant data structure\n",
    "    extracted_data = {\n",
    "        'title': title,\n",
    "        'publication_date': publication_date,\n",
    "        'author': author,\n",
    "        'content': content,\n",
    "        'figure_texts': figure_texts,\n",
    "        'table_data': table_data,  # Uncomment if you want to extract data from a table\n",
    "    }\n",
    "    return extracted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_estimation(extracted_data):\n",
    "    # Rule-based heuristics to quickly determine relevance\n",
    "    rule_based_score = 0\n",
    "    relevant_keywords = ['stock', 'market', 'finance', 'investment', 'earnings', 'dividend', 'shareholder', 'market trend']\n",
    "    for keyword in relevant_keywords:\n",
    "        if keyword.lower() in extracted_data['content'].lower():\n",
    "            rule_based_score = 1\n",
    "            break\n",
    "\n",
    "    # Use BERT for encoding the text content into contextualized word embeddings\n",
    "    model_name = 'bert-base-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    input_text = extracted_data['content']\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    # Prepare data for machine learning model\n",
    "    data = [embeddings]  # Use BERT embeddings as input for machine learning model\n",
    "    labels = [rule_based_score]\n",
    "\n",
    "    # Machine learning model for further refinement\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    xgb_model.fit(data, labels)\n",
    "\n",
    "    # Predict using the XGBoost classifier\n",
    "    xgb_score = xgb_model.predict(data)[0]\n",
    "\n",
    "    # Combine rule-based and XGBoost scores to get the final relevance score\n",
    "    relevance_score = max(rule_based_score, xgb_score)\n",
    "\n",
    "    return relevance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bert_embeddings(texts):\n",
    "    model_name = 'bert-base-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "\n",
    "def document_similarity(embeddings1, embeddings2):\n",
    "    similarity_score = cosine_similarity(embeddings1, embeddings2)\n",
    "    return similarity_score[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data(extracted_data, relevance_score):\n",
    "    # Create a connection to the SQLite database\n",
    "    conn = sqlite3.connect('financial_news.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create a table if it doesn't exist\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS financial_news (\n",
    "            title TEXT,\n",
    "            publication_date TEXT,\n",
    "            author TEXT,\n",
    "            content TEXT,\n",
    "            figure_texts TEXT,\n",
    "            table_data TEXT,\n",
    "            relevance_score INTEGER\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    # Insert the extracted data and relevance score into the database\n",
    "    cursor.execute('''\n",
    "        INSERT INTO financial_news (title, publication_date, author, content, figure_texts, table_data, relevance_score)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (extracted_data['title'], extracted_data['publication_date'], extracted_data['author'],\n",
    "          extracted_data['content'], '\\n'.join(extracted_data['figure_texts']),\n",
    "          '\\n'.join([','.join(row) for row in extracted_data['table_data']]), relevance_score))\n",
    "\n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_additional_urls(web_page_content):\n",
    "    # Parse the web page content using Scrapy's HtmlResponse\n",
    "    response = HtmlResponse(url='', body=web_page_content, encoding='utf-8')\n",
    "\n",
    "    # Extract URLs from anchor tags\n",
    "    additional_urls = response.css('a::attr(href)').extract()\n",
    "    # Filter and normalize URLs to include only absolute URLs starting with 'http'\n",
    "    additional_urls = [url for url in additional_urls if url.startswith('http')]\n",
    "\n",
    "    return additional_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autonomous_crawler(seed_urls):\n",
    "    visited_urls = set()\n",
    "    queue = deque(seed_urls)\n",
    "\n",
    "    while queue:\n",
    "        current_url = queue.popleft()\n",
    "\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Send a request to the URL and retrieve the web page content\n",
    "            response = requests.get(current_url)\n",
    "            if response.status_code == 200:\n",
    "                # Extract relevant information from the web page\n",
    "                extracted_data = extract_information(response.text)\n",
    "\n",
    "                # Apply the document relevance estimation using a large language model\n",
    "                relevance_score = relevance_estimation(extracted_data)\n",
    "\n",
    "                # Calculate BERT embeddings for the extracted content\n",
    "                content_embeddings = extract_bert_embeddings(extracted_data['content'])\n",
    "                \n",
    "                # Calculate document similarity with previously stored documents\n",
    "                conn = sqlite3.connect('financial_news.db')\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute('SELECT content FROM financial_news WHERE relevance_score=1')\n",
    "                stored_contents = cursor.fetchall()\n",
    "                conn.close()\n",
    "\n",
    "                # Check similarity with all relevant stored documents\n",
    "                similarity_scores = []\n",
    "                for stored_content in stored_contents:\n",
    "                    stored_content_embeddings = extract_bert_embeddings(stored_content[0])\n",
    "                    similarity = document_similarity(content_embeddings, stored_content_embeddings)\n",
    "                    similarity_scores.append(similarity)\n",
    "\n",
    "                # Check if any relevant documents are similar\n",
    "                if np.max(similarity_scores) > 0.8:\n",
    "                    relevance_score = 0  # Set to 0 if similarity threshold is met\n",
    "\n",
    "                # Store the extracted information and processed data\n",
    "                store_data(extracted_data, relevance_score)\n",
    "\n",
    "                # Extract and enqueue additional URLs from the web page for further crawling\n",
    "                additional_urls = extract_additional_urls(response.text)\n",
    "                queue.extend(additional_urls)\n",
    "\n",
    "                visited_urls.add(current_url)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions that might occur during crawling\n",
    "            print(f\"Error crawling URL: {current_url}\")\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_for_relevance_ranking():\n",
    "    # Load data from the database\n",
    "    conn = sqlite3.connect('financial_news.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT content, relevance_score FROM financial_news')\n",
    "    data = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    # Prepare feature vectors and target labels\n",
    "    texts = [row[0] for row in data]\n",
    "    relevance_scores = [row[1] for row in data]\n",
    "\n",
    "    # Calculate BERT embeddings for the content\n",
    "    embeddings = extract_bert_embeddings(texts)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(embeddings, relevance_scores, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the Random Forest Regression model\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the testing set\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model using Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "    # Return the trained model for relevance ranking\n",
    "    return rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuously_extend_model():\n",
    "    while True:\n",
    "        # Run the autonomous crawler to collect more web documents and data\n",
    "        csv_file = \"Dataset/input_urls.csv\"\n",
    "        seed_urls = read_seed_urls_from_csv(csv_file)\n",
    "        autonomous_crawler(seed_urls)\n",
    "\n",
    "        # Update the relevance ranking model\n",
    "        rf_model = build_model_for_relevance_ranking()\n",
    "\n",
    "        # Save the updated model for future use\n",
    "        with open('relevance_ranking_model.pkl', 'wb') as model_file:\n",
    "            pickle.dump(rf_model, model_file)\n",
    "\n",
    "        # Continue the loop to periodically extend the model\n",
    "        # You can adjust the time interval based on your specific requirements\n",
    "        time.sleep(3600)  # Wait for an hour before extending the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Call the continuously_extend_model() function\n",
    "    continuously_extend_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
